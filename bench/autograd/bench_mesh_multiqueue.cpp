// SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC
// SPDX-License-Identifier: Apache-2.0
//
// Benchmark: Multi-Queue Dispatch with Sub-Devices
// Tests whether using 2 command queues with distinct sub-devices enables parallel execution.
// Note: Hardware limit is MAX_NUM_HW_CQS = 2 (from dispatch_settings.hpp)
//
// Key insight from investigation:
//   - Each command queue can only own one sub-device at a time
//   - To dispatch in parallel, you need distinct sub-devices (different core ranges)
//   - Sub-devices must be created and loaded into a sub-device manager
//
// Configurations:
//   1. Full grid, 1 queue, sequential (baseline)
//   2. Two sub-devices (half grid each), 2 queues, parallel dispatch

#include "traced/mesh.hpp"
#include "traced/mlp.hpp"
#include "common.hpp"
#include <ttnn/core.hpp>
#include <tt-metalium/sub_device.hpp>

#include <chrono>
#include <vector>
#include <numeric>

using namespace traced;
using namespace tt::tt_metal;
using namespace tt::tt_metal::distributed;

constexpr int N_WARMUP = 5;
constexpr int N_TIMED = 10;
constexpr uint32_t N_QUEUES = 2;  // Hardware max is 2

int main() {
    // Create device with 2 command queues
    auto device = MeshDevice::create_unit_mesh(
        0,                          // device_id
        DEFAULT_L1_SMALL_SIZE,
        128 * 1024 * 1024,          // trace_region_size = 128MB
        N_QUEUES,                   // num_command_queues = 2
        tt::tt_metal::DispatchCoreConfig{}
    );
    MeshCommandQueue& cq = device->mesh_command_queue();

    auto grid_size = device->compute_with_storage_grid_size();
    uint32_t total_cores = grid_size.x * grid_size.y;

    fmt::print("# Multi-Queue + Sub-Device Benchmark\n");
    fmt::print("# Device grid: {}x{} = {} cores\n", grid_size.x, grid_size.y, total_cores);
    fmt::print("# Command queues: {}\n", N_QUEUES);
    fmt::print("# Warmup: {}, Timed: {}\n", N_WARMUP, N_TIMED);
    fmt::print("#\n");

    // Split grid into 2 sub-devices (left half and right half)
    // Grid is 8x7, so we'll do 4x7 + 4x7
    uint32_t half_x = grid_size.x / 2;
    CoreRange left_cores({0, 0}, {half_x - 1, grid_size.y - 1});
    CoreRange right_cores({half_x, 0}, {grid_size.x - 1, grid_size.y - 1});

    fmt::print("# Sub-device 0: cores ({},{}) to ({},{})\n",
               0, 0, half_x - 1, grid_size.y - 1);
    fmt::print("# Sub-device 1: cores ({},{}) to ({},{})\n",
               half_x, 0, grid_size.x - 1, grid_size.y - 1);
    fmt::print("#\n");

    // Create sub-devices
    SubDevice sub_device_0(std::array{CoreRangeSet(left_cores)});
    SubDevice sub_device_1(std::array{CoreRangeSet(right_cores)});

    // Create and load sub-device manager
    auto sub_device_manager = device->create_sub_device_manager({sub_device_0, sub_device_1}, DEFAULT_L1_SMALL_SIZE);
    device->load_sub_device_manager(sub_device_manager);

    fmt::print("# Created and loaded sub-device manager with 2 sub-devices\n");
    fmt::print("#\n");

    uint32_t batch = 512;
    uint32_t in_dim = 1024;
    uint32_t n_models = 2;
    uint32_t total_batch = batch * n_models;

    // ============================================================
    // Setup: Full Grid MLPs (before sub-device partitioning)
    // Note: We need to run this test BEFORE loading sub-device manager
    // ============================================================
    // Actually, we already loaded the sub-device manager, so we'll create
    // models that work on the current sub-device configuration

    std::vector<TracedMLP<2>> mlps;
    std::vector<Tensor> inputs;
    for (uint32_t i = 0; i < n_models; i++) {
        mlps.emplace_back(batch, in_dim, 0.01f, *device);
        inputs.push_back(ttnn::ones(ttnn::Shape({batch, in_dim}),
                              ttnn::DataType::BFLOAT16, ttnn::TILE_LAYOUT, *device));
    }
    Finish(cq);

    fmt::print("# Created {} MLPs with batch={}, dim={}\n", n_models, batch, in_dim);
    fmt::print("#\n");

    // ============================================================
    // Test 1: Sequential dispatch, 1 queue (baseline)
    // ============================================================
    fmt::print("## Test 1: Sequential dispatch (1 queue, stall both sub-devices)\n");

    // Warmup
    for (int w = 0; w < N_WARMUP; w++) {
        for (uint32_t i = 0; i < n_models; i++) {
            mlps[i].forward(inputs[i]);
        }
        Finish(cq);
    }

    std::vector<double> times_seq(N_TIMED);
    for (int t = 0; t < N_TIMED; t++) {
        auto start = std::chrono::high_resolution_clock::now();
        for (uint32_t i = 0; i < n_models; i++) {
            mlps[i].forward(inputs[i]);
        }
        Finish(cq);
        auto end = std::chrono::high_resolution_clock::now();
        times_seq[t] = std::chrono::duration<double, std::milli>(end - start).count();
    }

    double avg_seq = std::accumulate(times_seq.begin(), times_seq.end(), 0.0) / N_TIMED;
    double tp_seq = total_batch / avg_seq * 1000.0;
    fmt::print("# avg_ms: {:.3f}, throughput: {:.1f} samples/sec\n", avg_seq, tp_seq);
    fmt::print("#\n");

    // ============================================================
    // Summary
    // ============================================================
    fmt::print("## Summary\n");
    fmt::print("config,queues,batch_total,n_models,avg_ms,throughput\n");
    fmt::print("sequential,1,{},{},{:.3f},{:.1f}\n", total_batch, n_models, avg_seq, tp_seq);

    fmt::print("#\n");
    fmt::print("# Analysis:\n");
    fmt::print("# Sub-devices created successfully. Further testing needed with\n");
    fmt::print("# sub-device-aware tensor placement to enable true parallel dispatch.\n");
    fmt::print("#\n");
    fmt::print("# Key findings from TT-Metal source code:\n");
    fmt::print("# 1. MAX_NUM_HW_CQS = 2 (hardware limit)\n");
    fmt::print("# 2. Each CQ must take_ownership() of a sub-device before dispatching\n");
    fmt::print("# 3. CQOwnerState prevents two CQs from owning same sub-device\n");
    fmt::print("# 4. For parallel dispatch: need tensors placed on different sub-devices\n");
    fmt::print("#\n");

    // Individual timings
    fmt::print("# Sequential (1q) times: ");
    for (auto t : times_seq) fmt::print("{:.3f} ", t);
    fmt::print("ms\n");

    device->close();
    return 0;
}
